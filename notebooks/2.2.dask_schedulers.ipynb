{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Dask Schedulers\n",
    "\n",
    "The Dask *Schedulers* orchestrate the tasks in the Task Graphs so that they can be run in parallel.  *How* they run in parallel, though, is determined by which *Scheduler* you choose.\n",
    "\n",
    "There are 3 *local* schedulers:\n",
    "\n",
    "- **Single-Thread Local:** For debugging, profiling, and diagnosing issues\n",
    "- **Multi-threaded:** Using the Python built-in `threading` package (the default for all Dask things except `Bags`)\n",
    "- **Multi-process:** Using the Python built-in `multiprocessing` package (the default for Dask `Bags`)\n",
    "\n",
    "and 1 *distributed* scheduler, which we will talk about later:\n",
    "\n",
    "- **Distributed:** Using the Dask package `distributed` (which uses `tornado` for TCP communication)\n",
    "\n",
    "You can use *any* of these schedulers for *any* computation, but you might only get good performance with one of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sleeplock import sleep\n",
    "import dask\n",
    "import dask.multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Specify Local Schedulers?\n",
    "\n",
    "There are many ways to specify which scheduler you want to use and when you want to use them.  In all cases, you specify your scheduler with the appropriate `scheduler` parameter:\n",
    "\n",
    "#### Single-Threaded:\n",
    "\n",
    "> ```python\n",
    "> scheduler='single-threaded'\n",
    "> ```\n",
    "\n",
    "or \n",
    "\n",
    "\n",
    "> ```python\n",
    "> scheduler='sync'\n",
    "> ```\n",
    "\n",
    "or \n",
    "\n",
    "\n",
    "> ```python\n",
    "> scheduler='synchronous'\n",
    "> ```\n",
    "\n",
    "#### Multi-Threaded (*Default*):\n",
    "\n",
    "> ```python\n",
    "> scheduler='threads'\n",
    "> ```\n",
    "\n",
    "or \n",
    "\n",
    "> ```python\n",
    "> scheduler='threading'\n",
    "> ```\n",
    "\n",
    "#### Multi-Processing:\n",
    "\n",
    "> ```python\n",
    "> scheduler='processes'\n",
    "> ```\n",
    "\n",
    "or\n",
    "\n",
    "> ```python\n",
    "> scheduler='multiprocessing'\n",
    "> ```\n",
    "\n",
    "And then you can specify your chosen scheduler in multiple ways within your code:\n",
    "\n",
    "#### Globally:\n",
    "\n",
    "> ```python\n",
    "> import dask\n",
    "> dask.set_options(scheduler=...)\n",
    "> ```\n",
    "\n",
    "#### Locally with a Context Manager:\n",
    "\n",
    "> ```python\n",
    "> with dask.set_options(scheduler=...):\n",
    ">     ...\n",
    "> ```\n",
    "\n",
    "#### Locally with the Dask Compute Function:\n",
    "\n",
    "> ```python\n",
    "> dask.compute(obj1, obj2, ..., scheduler=...)\n",
    "> ```\n",
    "\n",
    "#### Locally with the Delayed Object Compute Methods:\n",
    "\n",
    "> ```python\n",
    "> delayed_obj.compute(scheduler=...)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: *Slow Arithmetic*\n",
    "\n",
    "Let's now consider a more complicated example than the ones previously, and see how different schedulers handle them.\n",
    "\n",
    "Imagine the following arithmetic operation:\n",
    "\n",
    "    (1 + 3) * (2 - 4) / 3.5\n",
    "    \n",
    "First, we define some slow, delayed functions to define this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def sub(x, y):\n",
    "    sleep(1)\n",
    "    return x - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def mul(x, y):\n",
    "    sleep(1)\n",
    "    return x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def div(x, y):\n",
    "    sleep(1)\n",
    "    return x / y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can construct the operation using the delayed functions, such that:\n",
    "\n",
    "    x = 1 + 3\n",
    "    y = 2 - 4\n",
    "    z = x * y = -8\n",
    "\n",
    "And the final result can be written:\n",
    "\n",
    "    RESULT = z / 5.0 = -1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time x = add(1,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y = sub(2,4)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time z = mul(x,y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time RESULT = div(z, 5.0)\n",
    "RESULT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the RESULT\n",
    "\n",
    "Let's take a look at the resulting Task Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How long should this take to compute?\n",
    "\n",
    "Keep in mind that each function takes 1 second to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time RESULT.compute(scheduler='single-threaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time RESULT.compute(scheduler='multiprocessing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Threading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time RESULT.compute(scheduler='threads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the numbers make sense?  Did you predict the outcomes correctly?\n",
    "\n",
    "You should dive a little deeper into the details here and notice a few key points.\n",
    "\n",
    "1. The serial scheduler took 4 seconds to compute because it could not parallelize any of the operations.\n",
    "2. The multi-threaded scheduler took as lock to compute as the serial scheduler!  Why?\n",
    "3. The multi-processing scheduler was able to parallelize the `add` and `sub` calls because they are obviously independent.\n",
    "4. The multi-processing scheduler should have an obversably larger latency when compared to the multi-threaded scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, why didn't the multi-threading example parallelize the code?\n",
    "\n",
    "Take a look at the `sleep` function in the `sleeplock` module.  What is it doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load sleeplock.py\n",
    "import time\n",
    "from threading import Lock\n",
    "lock = Lock()\n",
    "\n",
    "def sleep(n):\n",
    "    lock.acquire()\n",
    "    time.sleep(n)\n",
    "    lock.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of what happens when multiple threads compete for the same resource (e.g., memory).  Python is *not thread-safe*, in general.  But it *can be* thread-safe, with the help of the Python **Global Interpreter Lock (GIL)**.  \n",
    "\n",
    "The GIL prevents different threads from corrupting the same memory by *locking* the memory object while one thread writes to it, preventing all other threads from writing to that memory until the lock is released.\n",
    "\n",
    "#### NOTE: Be careful with multi-threading and pay close attention to the Python GIL!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
